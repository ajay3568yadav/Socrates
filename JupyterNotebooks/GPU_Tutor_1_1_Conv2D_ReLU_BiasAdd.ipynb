{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPU Tutor - 1 - 1_Conv2D_ReLU_BiasAdd\n\n",
        "This notebook demonstrates CPU and GPU acceleration in PyTorch.\n\n",
        "## Instructions\n",
        "1. Run each cell in order\n",
        "2. Make sure GPU is enabled: Runtime \u2192 Change runtime type \u2192 GPU\n",
        "3. The notebook will test both CPU and GPU performance using PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install numpy\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"No GPU available - will only run CPU tests\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CPU-Optimized PyTorch Code\n",
        "Below is the CPU implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def module_fn(\n",
        "    x: torch.Tensor,\n",
        "    conv_weight: torch.Tensor,\n",
        "    conv_bias: torch.Tensor,\n",
        "    bias: torch.Tensor,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Functional implementation of a neural network layer that:\n",
        "    1. Applies a 2D convolution with learnable weights and biases\n",
        "    2. Applies ReLU activation function\n",
        "    3. Adds a learnable bias term\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Input tensor of shape (N, C_in, H, W)\n",
        "        conv_weight (Tensor): Convolution weights of shape (C_out, C_in, kernel_size, kernel_size)\n",
        "        conv_bias (Tensor): Convolution bias of shape (C_out)\n",
        "        bias (Tensor): Additional bias term of shape (C_out, 1, 1)\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Output tensor of shape (N, C_out, H_out, W_out)\n",
        "    \"\"\"\n",
        "    x = F.conv2d(x, conv_weight, conv_bias)\n",
        "    x = torch.relu(x)\n",
        "    x = x + bias\n",
        "    return x\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple model that performs a convolution, applies ReLU, and adds a bias term.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):\n",
        "        super(Model, self).__init__()\n",
        "        conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n",
        "        self.conv_weight = nn.Parameter(conv.weight)\n",
        "        self.conv_bias = nn.Parameter(conv.bias)\n",
        "        self.bias = nn.Parameter(torch.randn(bias_shape) * 0.02)\n",
        "\n",
        "    def forward(self, x, fn=module_fn):\n",
        "        return fn(x, self.conv_weight, self.conv_bias, self.bias)\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "in_channels = 3\n",
        "out_channels = 16\n",
        "height, width = 32, 32\n",
        "kernel_size = 3\n",
        "bias_shape = (out_channels, 1, 1)\n",
        "\n",
        "\n",
        "def get_inputs():\n",
        "    return [torch.randn(batch_size, in_channels, height, width)]\n",
        "\n",
        "\n",
        "def get_init_inputs():\n",
        "    return [in_channels, out_channels, kernel_size, bias_shape]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU-Accelerated PyTorch Code\n",
        "Below is the GPU implementation generated from the CPU code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def module_fn(\n",
        "    x: torch.Tensor,\n",
        "    conv_weight: torch.Tensor,\n",
        "    conv_bias: torch.Tensor,\n",
        "    bias: torch.Tensor,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Functional implementation of a neural network layer that:\n",
        "    1. Applies a 2D convolution with learnable weights and biases\n",
        "    2. Applies ReLU activation function\n",
        "    3. Adds a learnable bias term\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Input tensor of shape (N, C_in, H, W)\n",
        "        conv_weight (Tensor): Convolution weights of shape (C_out, C_in, kernel_size, kernel_size)\n",
        "        conv_bias (Tensor): Convolution bias of shape (C_out)\n",
        "        bias (Tensor): Additional bias term of shape (C_out, 1, 1)\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Output tensor of shape (N, C_out, H_out, W_out)\n",
        "    \"\"\"\n",
        "    x = F.conv2d(x, conv_weight, conv_bias)\n",
        "    x = torch.relu(x)\n",
        "    x = x + bias\n",
        "    return x\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple model that performs a convolution, applies ReLU, and adds a bias term.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):\n",
        "        super(Model, self).__init__()\n",
        "        conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n",
        "        self.conv_weight = nn.Parameter(conv.weight)\n",
        "        self.conv_bias = nn.Parameter(conv.bias)\n",
        "        self.bias = nn.Parameter(torch.randn(bias_shape) * 0.02)\n",
        "\n",
        "    def forward(self, x, fn=module_fn):\n",
        "        return fn(x, self.conv_weight, self.conv_bias, self.bias)\n",
        "\n",
        "batch_size = 128\n",
        "in_channels = 3\n",
        "out_channels = 16\n",
        "height, width = 32, 32\n",
        "kernel_size = 3\n",
        "bias_shape = (out_channels, 1, 1)\n",
        "\n",
        "def get_inputs():\n",
        "    return [torch.randn(batch_size, in_channels, height, width).to(device)]\n",
        "\n",
        "def get_init_inputs():\n",
        "    return [in_channels, out_channels, kernel_size, bias_shape]\n",
        "\n",
        "# Initialize model and move it to the GPU\n",
        "model = Model(*get_init_inputs()).to(device)\n",
        "\n",
        "# Get inputs and move them to the GPU\n",
        "inputs = get_inputs()\n",
        "\n",
        "# Forward pass\n",
        "output = model(*inputs)\n",
        "\n",
        "# Print the output shape\n",
        "print(output.shape)\n",
        "```\n",
        "\n",
        "### Summary\n",
        "This code ensures that both the model and the inputs are on the GPU, leading to efficient computation. By checking for GPU availability and using the `.to(device)` method, it ensures portability and optimal performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Test Inputs\n",
        "This cell generates test inputs for the function. Adjust as needed for your operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Example for matrix multiplication (adjust as needed)\n",
        "N = 512\n",
        "A = torch.randn(N, N)\n",
        "B = torch.randn(N, N)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CPU Performance Test\n",
        "Run and time the CPU code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "result_cpu = module_fn(A, B)  # Adjust arguments as needed\n",
        "cpu_time = time.time() - start\n",
        "print(f'CPU result shape: {result_cpu.shape}')\n",
        "print(f'CPU execution time: {cpu_time:.6f} seconds')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Performance Test\n",
        "Run and time the GPU code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    A_gpu = A.cuda()\n",
        "    B_gpu = B.cuda()\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    result_gpu = module_fn_gpu(A_gpu, B_gpu)  # Adjust arguments as needed\n",
        "    torch.cuda.synchronize()\n",
        "    gpu_time = time.time() - start\n",
        "    print(f'GPU result shape: {result_gpu.shape}')\n",
        "    print(f'GPU execution time: {gpu_time:.6f} seconds')\n",
        "    print(f'Speedup: {cpu_time/gpu_time:.2f}x')\n",
        "else:\n",
        "    print('CUDA not available - skipping GPU test')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Usage Comparison\n",
        "Compare memory usage for CPU and GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import psutil\n",
        "import os\n",
        "# CPU memory usage\n",
        "process = psutil.Process(os.getpid())\n",
        "cpu_mem = process.memory_info().rss / 1e6\n",
        "print(f'CPU memory usage: {cpu_mem:.2f} MB')\n",
        "\n",
        "# GPU memory usage\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    A_gpu = A.cuda()\n",
        "    B_gpu = B.cuda()\n",
        "    torch.cuda.synchronize()\n",
        "    result_gpu = module_fn_gpu(A_gpu, B_gpu)\n",
        "    torch.cuda.synchronize()\n",
        "    gpu_mem = torch.cuda.max_memory_allocated() / 1e6\n",
        "    print(f'GPU memory usage: {gpu_mem:.2f} MB')\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    print('CUDA not available for memory analysis')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "GPU_Tutor_1_1_Conv2D_ReLU_BiasAdd",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}